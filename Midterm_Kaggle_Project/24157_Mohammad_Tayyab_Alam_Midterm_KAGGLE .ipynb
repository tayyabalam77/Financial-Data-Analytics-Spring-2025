{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b157a0",
   "metadata": {
    "id": "f7b157a0"
   },
   "source": [
    "# Mohammad Tayyab Alam 24157 Midterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d553e5",
   "metadata": {
    "id": "04d553e5"
   },
   "source": [
    "## This notebook outlines the best performing model which achieved an accuracy of 95.338 % on the kaggle leaderboard.\n",
    "### Objective\n",
    "The goal of this project is to build a predictive model to identify overstressed customers in a microfinance setting. This will help financial institutions intervene early and avoid loan defaults. The dataset contains customer financial data, and the target variable `Y` indicates whether the customer is overstressed (1) or not (0).\n",
    "\n",
    "# NOTE:\n",
    "Throughout this project my entire rationale and thinking was based on achieving the highest score possible and dedicating time and effort to explore the parameters for the best machine learning model. This project was taken in two phases\n",
    "\n",
    "\n",
    "\n",
    "1.   Phase 1 -> Testing out all 10 models on raw data with no feature engineering and bare minimum missing value imputation and scaling to even the playing field with no intense hyper parameter tuning. At the end of this phase models that struggled were eliminated and only xgboost , lgboost , catboost and random forest survived.\n",
    "2.   Phase 2 -> The best performing models were intensely iterated to find the best hyperparameters via random search and feature engineering was also explored and regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8031b8b7",
   "metadata": {
    "id": "8031b8b7"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c959603f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c959603f",
    "outputId": "26e0dac3-47b2-4ab7-c067-d39733c331cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   ID  Income index  income_volatility  employment_status_flag  \\\n",
       " 0   1     87.000000          34.118411                       0   \n",
       " 1   2     82.372284          31.573280                       0   \n",
       " 2   3     50.000000          27.771653                       0   \n",
       " 3   4     66.236109          26.515922                       0   \n",
       " 4   5     81.303299          20.843691                       0   \n",
       " \n",
       "    dependents_count  gender_flag     Payment  collateral_flag  \\\n",
       " 0                 2            0  165.100000                1   \n",
       " 1                 0            1  162.983897                1   \n",
       " 2                 0            1  165.100000                1   \n",
       " 3                 0            1  167.009549                1   \n",
       " 4                 0            1  158.165419                0   \n",
       " \n",
       "    repayment_history_score  missed_payment_count  ...       X70  X71  X72  \\\n",
       " 0                      829                     2  ...  0.040000  0.0  0.0   \n",
       " 1                      724                     0  ...  0.033431  0.0  0.0   \n",
       " 2                      895                     2  ...  0.010000  0.0  0.0   \n",
       " 3                      637                     0  ...  0.039363  0.0  0.0   \n",
       " 4                      564                     0  ...  0.069242  0.0  0.0   \n",
       " \n",
       "    X73  X74  X75  X76  X77  X78  Y  \n",
       " 0  0.0  0.0  0.0  0.0  0.0  0.0  0  \n",
       " 1  0.0  0.0  0.0  0.0  0.0  0.0  0  \n",
       " 2  0.0  0.0  0.0  0.0  0.0  0.0  0  \n",
       " 3  0.0  0.0  0.0  0.0  0.0  0.0  0  \n",
       " 4  0.0  0.0  0.0  0.0  0.0  0.0  0  \n",
       " \n",
       " [5 rows x 79 columns],\n",
       "        ID  Income index  income_volatility  employment_status_flag  \\\n",
       " 0  200001     84.073751          28.503284                       0   \n",
       " 1  200002     72.769709          23.093559                       0   \n",
       " 2  200003     73.983018          33.235401                       0   \n",
       " 3  200004     67.508735          38.790778                       0   \n",
       " 4  200005     60.078325          31.754265                       0   \n",
       " \n",
       "    dependents_count  gender_flag     Payment  collateral_flag  \\\n",
       " 0                 0            0  180.939439                1   \n",
       " 1                 3            1  166.598698                1   \n",
       " 2                 0            1  176.976489                1   \n",
       " 3                 0            0  176.055848                0   \n",
       " 4                 1            1  166.217227                4   \n",
       " \n",
       "    repayment_history_score  missed_payment_count  ...       X69       X70  \\\n",
       " 0                      703                     0  ...  0.073087  0.031544   \n",
       " 1                      560                     0  ...  0.078187  0.041471   \n",
       " 2                      481                     0  ...  0.368555  0.216006   \n",
       " 3                      557                     0  ...  0.010000  0.007705   \n",
       " 4                      297                     0  ...  0.040525  0.017631   \n",
       " \n",
       "    X71  X72       X73  X74  X75  X76  X77  X78  \n",
       " 0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  \n",
       " 1  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  \n",
       " 2  0.0  0.0  0.890935  0.0  0.0  0.0  0.0  0.0  \n",
       " 3  0.0  0.0  1.000000  0.0  0.0  0.0  0.0  0.0  \n",
       " 4  0.0  0.0  0.236867  0.0  0.0  0.0  0.0  0.0  \n",
       " \n",
       " [5 rows x 78 columns])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv('fda_trainingset.csv')\n",
    "test_df = pd.read_csv('fda_testset.csv')\n",
    "\n",
    "# Check the first few rows of the training and test datasets\n",
    "train_df.head(), test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5021e162",
   "metadata": {
    "id": "5021e162"
   },
   "source": [
    "### Missing Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "428387d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "428387d1",
    "outputId": "6d545d84-0904-4356-90cc-4cb8dbcbaebe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Missing Count  Missing Percentage\n",
      "Income index                2112              1.0560\n",
      "income_volatility           1720              0.8600\n",
      "X76                          371              0.1855\n",
      "X78                          367              0.1835\n",
      "X77                          360              0.1800\n",
      "X75                          357              0.1785\n"
     ]
    }
   ],
   "source": [
    "# Calculate missing values\n",
    "missing_counts =train_df.isnull().sum()\n",
    "missing_percent = (missing_counts / len(train_df)) * 100\n",
    "\n",
    "# Combine into a summary DataFrame\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Missing Percentage': missing_percent\n",
    "})\n",
    "\n",
    "# Filter out columns with no missing values\n",
    "missing_summary = missing_summary[missing_summary['Missing Count'] > 0]\n",
    "\n",
    "# Sort by most missing\n",
    "missing_summary = missing_summary.sort_values(by='Missing Count', ascending=False)\n",
    "\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5aeb9bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5aeb9bf",
    "outputId": "2e2b0e30-541e-4115-ec39-3e39a93e7b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution (Counts):\n",
      "0    199475\n",
      "1       525\n",
      "Name: Y, dtype: int64\n",
      "\n",
      "Class Distribution (Percentages):\n",
      "0    99.7375\n",
      "1     0.2625\n",
      "Name: Y, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution in column 'Y'\n",
    "class_distribution = train_df['Y'].value_counts()\n",
    "\n",
    "# Calculate percentages for each class\n",
    "class_percentage = train_df['Y'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Display class counts and percentages\n",
    "print(\"Class Distribution (Counts):\")\n",
    "print(class_distribution)\n",
    "print(\"\\nClass Distribution (Percentages):\")\n",
    "print(class_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d05c1",
   "metadata": {
    "id": "3f9d05c1"
   },
   "source": [
    "# Data Preprocessing and Feature Engineering\n",
    "\n",
    "We'll first handle any missing values and standardize the data for better model performance. We'll also create additional features like the sum, mean, and standard deviation of the features to capture more information from the data. These simple features helped increase the score from 0.93 to 0.95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12a5f128",
   "metadata": {
    "id": "12a5f128"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate features and target\n",
    "X = train_df.drop(columns=['Y'])\n",
    "y = train_df['Y'].astype(pd.Int64Dtype()).fillna(0)\n",
    "\n",
    "test_ids = test_df['ID']  # Save IDs\n",
    "\n",
    "# Combine train and test data for consistent preprocessing\n",
    "combined = pd.concat([X, test_df.drop(columns=['ID'])], axis=0)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "combined_imputed = pd.DataFrame(imputer.fit_transform(combined), columns=combined.columns)\n",
    "\n",
    "# Feature engineering\n",
    "combined_imputed['feature_sum'] = combined_imputed.sum(axis=1)\n",
    "combined_imputed['feature_mean'] = combined_imputed.mean(axis=1)\n",
    "combined_imputed['feature_std'] = combined_imputed.std(axis=1)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "combined_scaled = pd.DataFrame(scaler.fit_transform(combined_imputed), columns=combined_imputed.columns)\n",
    "\n",
    "# Split back into train and test sets\n",
    "X_processed = combined_scaled.iloc[:len(X)]\n",
    "test_processed = combined_scaled.iloc[len(X):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c336f9ce",
   "metadata": {
    "id": "c336f9ce"
   },
   "source": [
    "### Rechecking Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b6220d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84b6220d",
    "outputId": "91fa6c3d-2916-448d-fe60-6c30c1b97139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Missing Count, Missing Percentage]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Calculate missing values\n",
    "missing_counts =X_processed.isnull().sum()\n",
    "missing_percent = (missing_counts / len(X_processed)) * 100\n",
    "\n",
    "# Combine into a summary DataFrame\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Missing Percentage': missing_percent\n",
    "})\n",
    "\n",
    "# Filter out columns with no missing values\n",
    "missing_summary = missing_summary[missing_summary['Missing Count'] > 0]\n",
    "\n",
    "# Sort by most missing\n",
    "missing_summary = missing_summary.sort_values(by='Missing Count', ascending=False)\n",
    "\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b66aa",
   "metadata": {
    "id": "f41b66aa"
   },
   "source": [
    "# Model Selection and Hyper Parameter Tuning\n",
    "## Throughout this project, around 80 submissions were done to determine the best possible model. Initial submissions focussed on submitting models like:\n",
    "| Model               | Best Score     | Remarks                                                                 |\n",
    "|---------------------|----------------|-------------------------------------------------------------------------|\n",
    "| Decision Tree       | 0.51470         | Model was discarded due to low performance                             |\n",
    "| KNN                 | 0.55477         | Model was discarded due to low performance                             |\n",
    "| Logistic Regression | 0.51836         | Model was discarded due to low performance                             |\n",
    "| XGBOOST             | **0.95338**     | Best model with significant hyperparameter tuning                      |\n",
    "| CATBOOST            | ~0.94           | Strong performance, but did not beat XGBOOST                           |\n",
    "| LGBOOST             | ~0.93           | Good performance, but behind CATBOOST and XGBOOST                      |\n",
    "| STACKING            | ~0.95           | Ensemble of LG, XG, and CATBOOST; strong but still behind XGBOOST      |\n",
    "| ADABOOST            | ~0.52           | Model was discarded due to low performance                             |\n",
    "| GRADIENT DESCENT    | ~0.50           | Model was discarded due to low performance                             |\n",
    "| RANDOM FOREST       | ~0.84           | First strong model found                                               |\n",
    "| NAIVE BAYES         | ~0.50           | Model was discarded due to low performance                             |\n",
    "\n",
    "\n",
    "### When submitting these models it quickly became clear that basic models like Decision Tree , Logistic Regeression , NAIVE Bayes struggle in this problem set. This can be attributed to the fact that there are 79 columns and severe class imbalance of 525 (1) out of 200,000 rows.\n",
    "### So the next best approach was to move to these complex models like LGBOOST , XGBOOST , CATBOOST and ADABOOST and to use stacking ensembling if required.\n",
    "\n",
    "#### When investigating the kaggle submissions the strategy used was that if a model scores low on initial modelling with missing values imputed and data scaled and class imbalance being handled via SMOTE then the model was discarded due to time constraints and the competitive nature. Once best models were identified ( LGBOOST , CATBOOST , XGBOOST and STACKING of these 3) significant efforts were directed to improving the score of these via feature engineering and and hyper parameter tuning via random search instead of grid search due to computational limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a231e60",
   "metadata": {
    "id": "6a231e60"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# After more than 60 submissions these are the best parameters for the XGBOOST model.\n",
    "## Hyperparameter Tuning with XGBoost and Insights Gained\n",
    "\n",
    "### **1. Hyperparameters in XGBoost Model**\n",
    "\n",
    "In this section, we define and fine-tune several **hyperparameters** of the XGBoost model using **RandomizedSearchCV**. Here’s an explanation of what each parameter means and how adjusting them can impact model performance:\n",
    "\n",
    "- **n_estimators**:\n",
    "    - This defines the number of trees (or boosting rounds) the model will use. More trees often lead to better performance but can also cause overfitting if not managed properly. Initially, when I only increased the number of trees (iterations), I observed that the model started to overfit. This taught me that simply increasing iterations isn't enough — careful tuning of other parameters is also crucial.\n",
    "\n",
    "- **learning_rate**:\n",
    "    - The learning rate determines the size of the steps the model takes during training. A smaller learning rate leads to more conservative updates and requires more trees (`n_estimators`) to converge. I tested values like `0.0025` to ensure that the model gradually learns without skipping important details, especially when combined with a higher number of trees.\n",
    "\n",
    "- **max_depth**:\n",
    "    - This parameter controls the maximum depth of the decision trees. A deeper tree can capture more complex relationships in the data, but if set too high, it risks overfitting by capturing noise in the training data. I tested values from `5` to `8` and found that setting a reasonable depth (like `7`) led to better generalization.\n",
    "\n",
    "- **min_child_weight**:\n",
    "    - This defines the minimum sum of instance weights (hessian) in a child node. This parameter is used to control overfitting. Higher values prevent the model from learning overly specific patterns that might not generalize well. I found that increasing it helped reduce overfitting when tuning my model.\n",
    "\n",
    "- **gamma**:\n",
    "    - Gamma is the minimum loss reduction required to make a further partition on a leaf node. Essentially, it prevents splits that would result in a minimal improvement. When set too low, the model may create overly complex trees. I experimented with values between `0.3` and `0.6` to ensure the model wasn't too sensitive to small changes.\n",
    "\n",
    "- **subsample**:\n",
    "    - This parameter controls the fraction of training data used to grow each tree. It helps prevent overfitting by introducing randomness into the model. I set it to `0.85`, which means 85% of the training data was used for each tree, leaving room for variation without sacrificing accuracy.\n",
    "\n",
    "- **colsample_bytree**:\n",
    "    - Similar to `subsample`, this controls the fraction of features used to grow each tree. It is another way to reduce overfitting. I set it to `0.85`, ensuring each tree sees a different subset of features to avoid overfitting on specific attributes.\n",
    "\n",
    "- **reg_alpha** (L1 regularization):\n",
    "    - L1 regularization helps in feature selection by shrinking some feature coefficients to zero. This is particularly useful when there are many features and some are not contributing significantly to the model. This was the key insight I gained — after applying L1 regularization (Lasso), my model's score improved drastically. I learned that Lasso regularization helped simplify the model by removing irrelevant features, leading to better generalization.\n",
    "\n",
    "- **reg_lambda** (L2 regularization):\n",
    "    - L2 regularization penalizes large coefficients but does not shrink them to zero. It helps prevent overfitting by discouraging overly complex models. I experimented with values like `2.0` and found that the combination of L1 and L2 regularization helped balance model complexity and accuracy.\n",
    "\n",
    "- **objective**:\n",
    "    - This specifies the learning task and objective function. For binary classification, the correct value is `'binary:logistic'`, which outputs probabilities for the positive class.\n",
    "\n",
    "- **eval_metric**:\n",
    "    - This determines how the model's performance is evaluated during training. The `logloss` metric is suitable for binary classification tasks, as it penalizes incorrect classifications based on their confidence (probability).\n",
    "\n",
    "- **tree_method**:\n",
    "    - This defines the algorithm used for tree construction. The `hist` method is particularly useful for large datasets as it is faster than other methods, like `auto` or `exact`.\n",
    "\n",
    "### **2. Key Insights and Learnings**\n",
    "\n",
    "- **Overfitting and Iterations**:\n",
    "    - Initially, I only focused on increasing the number of trees (`n_estimators`), thinking it would improve accuracy. However, this led to overfitting. This experience taught me that adding more iterations or trees is not always beneficial — regularization and other parameters must also be tuned to prevent the model from becoming too complex.\n",
    "\n",
    "- **Importance of Regularization (Lasso - L1)**:\n",
    "    - The most significant insight I gained was the impact of **Lasso regularization** (through **reg_alpha**). After using it, my model's performance improved dramatically. This aligned with what I learned in class: **too many features are not good**. Rather than blindly using all available features, **feature engineering** is key. By focusing on the right features (like the **sum, mean, and standard deviation**), I improved my score from **0.93 to 0.95**.\n",
    "\n",
    "- **Feature Engineering**:\n",
    "    - By adding engineered features like `feature_sum`, `feature_mean`, and `feature_std`, I was able to provide the model with more useful information. This showed me that **feature engineering is crucial** for model performance. It's not just about having more features; it's about having the **right features** that contribute meaningfully to predictions. This approach helped the model generalize better and significantly improved performance.\n",
    "\n",
    "### **3. Conclusion**\n",
    "\n",
    "Through this exercise, I learned the importance of hyperparameter tuning, particularly in relation to regularization, and the impact of effective feature engineering. The experience validated key lessons from class, such as the importance of **feature selection**, **regularization** to prevent overfitting, and the fact that **more features are not always better** — the right features are the key to improving model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f60b2a21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f60b2a21",
    "outputId": "e983a80b-df8d-4124-a515-71137842a558"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "14 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 1663, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 628, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 1137, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1614, in __init__\n",
      "    self._init(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1678, in _init\n",
      "    it.reraise()\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 572, in reraise\n",
      "    raise exc  # pylint: disable=raising-bad-type\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 553, in _handle_exception\n",
      "    return fn()\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 640, in <lambda>\n",
      "    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1654, in next\n",
      "    input_data(**self.kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 629, in input_data\n",
      "    self.proxy.set_info(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 961, in set_info\n",
      "    self.set_label(label)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1099, in set_label\n",
      "    dispatch_meta_backend(self, label, \"label\", \"float\")\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1603, in dispatch_meta_backend\n",
      "    _meta_from_pandas_series(data, name, dtype, handle)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 713, in _meta_from_pandas_series\n",
      "    _meta_from_numpy(data, name, dtype, handle)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1533, in _meta_from_numpy\n",
      "    _check_call(_LIB.XGDMatrixSetInfoFromInterface(handle, c_str(field), interface_str))\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [03:33:59] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\data\\array_interface.cu:44: Check failed: err == cudaGetLastError() (0 vs. 46) : \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 1663, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 628, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 1137, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1614, in __init__\n",
      "    self._init(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1678, in _init\n",
      "    it.reraise()\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 572, in reraise\n",
      "    raise exc  # pylint: disable=raising-bad-type\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 553, in _handle_exception\n",
      "    return fn()\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 640, in <lambda>\n",
      "    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1654, in next\n",
      "    input_data(**self.kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 629, in input_data\n",
      "    self.proxy.set_info(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 961, in set_info\n",
      "    self.set_label(label)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1099, in set_label\n",
      "    dispatch_meta_backend(self, label, \"label\", \"float\")\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1603, in dispatch_meta_backend\n",
      "    _meta_from_pandas_series(data, name, dtype, handle)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 713, in _meta_from_pandas_series\n",
      "    _meta_from_numpy(data, name, dtype, handle)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1533, in _meta_from_numpy\n",
      "    _check_call(_LIB.XGDMatrixSetInfoFromInterface(handle, c_str(field), interface_str))\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [03:34:02] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\data\\array_interface.cu:44: Check failed: err == cudaGetLastError() (0 vs. 46) : \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 1663, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 628, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 1137, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1614, in __init__\n",
      "    self._init(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1678, in _init\n",
      "    it.reraise()\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 572, in reraise\n",
      "    raise exc  # pylint: disable=raising-bad-type\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 553, in _handle_exception\n",
      "    return fn()\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 640, in <lambda>\n",
      "    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1654, in next\n",
      "    input_data(**self.kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 629, in input_data\n",
      "    self.proxy.set_info(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 961, in set_info\n",
      "    self.set_label(label)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1099, in set_label\n",
      "    dispatch_meta_backend(self, label, \"label\", \"float\")\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1603, in dispatch_meta_backend\n",
      "    _meta_from_pandas_series(data, name, dtype, handle)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 713, in _meta_from_pandas_series\n",
      "    _meta_from_numpy(data, name, dtype, handle)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1533, in _meta_from_numpy\n",
      "    _check_call(_LIB.XGDMatrixSetInfoFromInterface(handle, c_str(field), interface_str))\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [03:34:01] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\data\\array_interface.cu:44: Check failed: err == cudaGetLastError() (0 vs. 46) : \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 1663, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 628, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 1137, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1614, in __init__\n",
      "    self._init(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1678, in _init\n",
      "    it.reraise()\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 572, in reraise\n",
      "    raise exc  # pylint: disable=raising-bad-type\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 553, in _handle_exception\n",
      "    return fn()\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 640, in <lambda>\n",
      "    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1654, in next\n",
      "    input_data(**self.kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 629, in input_data\n",
      "    self.proxy.set_info(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 961, in set_info\n",
      "    self.set_label(label)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1099, in set_label\n",
      "    dispatch_meta_backend(self, label, \"label\", \"float\")\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1603, in dispatch_meta_backend\n",
      "    _meta_from_pandas_series(data, name, dtype, handle)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 713, in _meta_from_pandas_series\n",
      "    _meta_from_numpy(data, name, dtype, handle)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1533, in _meta_from_numpy\n",
      "    _check_call(_LIB.XGDMatrixSetInfoFromInterface(handle, c_str(field), interface_str))\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [03:34:03] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\data\\array_interface.cu:44: Check failed: err == cudaGetLastError() (0 vs. 46) : \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 1663, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 628, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 1137, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1614, in __init__\n",
      "    self._init(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1678, in _init\n",
      "    it.reraise()\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 572, in reraise\n",
      "    raise exc  # pylint: disable=raising-bad-type\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 553, in _handle_exception\n",
      "    return fn()\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 640, in <lambda>\n",
      "    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1654, in next\n",
      "    input_data(**self.kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 629, in input_data\n",
      "    self.proxy.set_info(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 961, in set_info\n",
      "    self.set_label(label)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1099, in set_label\n",
      "    dispatch_meta_backend(self, label, \"label\", \"float\")\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1603, in dispatch_meta_backend\n",
      "    _meta_from_pandas_series(data, name, dtype, handle)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 713, in _meta_from_pandas_series\n",
      "    _meta_from_numpy(data, name, dtype, handle)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1533, in _meta_from_numpy\n",
      "    _check_call(_LIB.XGDMatrixSetInfoFromInterface(handle, c_str(field), interface_str))\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [03:34:04] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\data\\array_interface.cu:44: Check failed: err == cudaGetLastError() (0 vs. 46) : \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 1663, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 628, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py\", line 1137, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1614, in __init__\n",
      "    self._init(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1678, in _init\n",
      "    it.reraise()\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 572, in reraise\n",
      "    raise exc  # pylint: disable=raising-bad-type\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 553, in _handle_exception\n",
      "    return fn()\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 640, in <lambda>\n",
      "    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1654, in next\n",
      "    input_data(**self.kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 629, in input_data\n",
      "    self.proxy.set_info(\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 961, in set_info\n",
      "    self.set_label(label)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 1099, in set_label\n",
      "    dispatch_meta_backend(self, label, \"label\", \"float\")\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1603, in dispatch_meta_backend\n",
      "    _meta_from_pandas_series(data, name, dtype, handle)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 713, in _meta_from_pandas_series\n",
      "    _meta_from_numpy(data, name, dtype, handle)\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\data.py\", line 1533, in _meta_from_numpy\n",
      "    _check_call(_LIB.XGDMatrixSetInfoFromInterface(handle, c_str(field), interface_str))\n",
      "  File \"C:\\Users\\Tayyab\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [03:34:05] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\data\\array_interface.cu:44: Check failed: err == cudaGetLastError() (0 vs. 46) : \n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters from RandomizedSearchCV: {'subsample': 0.85, 'reg_lambda': 1.5, 'reg_alpha': 0.6, 'n_estimators': 2000, 'min_child_weight': 8, 'max_depth': 7, 'learning_rate': 0.005, 'gamma': 0.6, 'colsample_bytree': 0.7}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define the base XGBoost model\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=3000,\n",
    "    learning_rate=0.005,\n",
    "    max_depth=7,\n",
    "    min_child_weight=8,\n",
    "    gamma=0.5,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    reg_alpha=0.6,\n",
    "    reg_lambda=2.0,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    tree_method='hist',\n",
    "    random_state=42,\n",
    "    verbosity=1\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning with RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'learning_rate': [0.001, 0.005, 0.01, 0.05],\n",
    "    'max_depth': [5, 6, 7, 8],\n",
    "    'min_child_weight': [5, 6, 7, 8],\n",
    "    'gamma': [0.3, 0.4, 0.5, 0.6],\n",
    "    'subsample': [0.7, 0.75, 0.8, 0.85],\n",
    "    'colsample_bytree': [0.7, 0.75, 0.8, 0.85],\n",
    "    'reg_alpha': [0.4, 0.5, 0.6],\n",
    "    'reg_lambda': [1.5, 2.0, 2.5],\n",
    "    'n_estimators': [2000, 3000],\n",
    "}\n",
    "\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(xgb_model, param_distributions=param_dist, n_iter=20,\n",
    "                                   scoring='roc_auc', cv=3, verbose=3, random_state=42, n_jobs=-1)\n",
    "\n",
    "random_search.fit(X_processed, y)\n",
    "\n",
    "# Display best hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(f\"\\nBest Parameters from RandomizedSearchCV: {random_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc1c233",
   "metadata": {
    "id": "7cc1c233"
   },
   "source": [
    "# Best Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0169a1",
   "metadata": {
    "id": "2b0169a1"
   },
   "source": [
    "Best Parameters from RandomizedSearchCV: {'subsample': 0.85, 'reg_lambda': 1.5, 'reg_alpha': 0.6, 'n_estimators': 2000, 'min_child_weight': 8, 'max_depth': 7, 'learning_rate': 0.005, 'gamma': 0.6, 'colsample_bytree': 0.7}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8057155e",
   "metadata": {
    "id": "8057155e"
   },
   "source": [
    "# Model Training with Cross-Validation\n",
    "\n",
    "## Thought Process and Key Insights\n",
    "\n",
    "In this section, I implemented **Stratified K-Fold Cross-Validation** and evaluated the model using **AUC-ROC**. Here are the key points I learned and the reasoning behind these choices:\n",
    "\n",
    "### **1. Stratified K-Fold Cross-Validation**:\n",
    "- **Stratified K-Fold** is used to ensure that each fold has the same distribution of the target classes (i.e., the same proportion of positive and negative samples) as the overall dataset. This is particularly important in cases of **class imbalance** where random splitting might lead to folds with an unequal class distribution.\n",
    "- By using **n_splits=5**, I split the data into 5 parts (folds), ensuring that each fold is used for validation exactly once. This allows the model to be trained and validated on multiple different subsets of the data, providing a better estimate of its generalization performance.\n",
    "- **Shuffling** the data before splitting it ensures randomness and avoids potential bias from ordering in the dataset, which can be especially important when the data is ordered in some way (e.g., time series).\n",
    "\n",
    "### **2. Out-of-Fold Predictions (OOF)**:\n",
    "- The **OOF (Out-of-Fold) predictions** are made by training the model on all but one fold and predicting on the held-out fold. This approach allows for an evaluation of how the model performs on unseen data in each fold, providing an estimate of how the model generalizes to new data.\n",
    "- I used **OOF predictions** to evaluate the model's performance during training. This was the first time I encountered the concept of **OOF predictions**. I learned that this technique helps in obtaining more reliable performance metrics since it ensures that the validation data has not been seen by the model during training.\n",
    "- OOF predictions also help in avoiding **overfitting**, as the model’s performance is evaluated on data that it hasn't seen during training.\n",
    "\n",
    "### **3. Averaging Predictions Across Folds**:\n",
    "- After each fold, I made predictions on the **test set**, and the predictions were averaged across all folds. This helps in reducing variance and ensures that the final prediction is more stable.\n",
    "- Averaging the predictions from multiple models (from different folds) helps in improving accuracy and reducing the likelihood of overfitting to any particular fold.\n",
    "\n",
    "### **4. AUC-ROC for Model Evaluation**:\n",
    "- I chose **AUC-ROC** as the evaluation metric because it is a reliable measure for binary classification, especially when dealing with imbalanced datasets. The **AUC (Area Under the Curve)** tells us how well the model distinguishes between the two classes.\n",
    "- **AUC-ROC** provides a clear view of the model's performance across all possible classification thresholds, making it a valuable tool for evaluating models on imbalanced datasets.\n",
    "\n",
    "### **Key Insights I Gained**:\n",
    "- **Learning About OOF**: This project was my first experience working with **OOF predictions**. I now understand that OOF predictions allow me to evaluate the model on data that it hasn’t seen during training, giving a better estimate of how the model will perform on unseen data. It also helps in mitigating overfitting.\n",
    "- **Stratified K-Fold**: Using Stratified K-Fold cross-validation helped me ensure that the class distribution remained consistent across all folds, which is critical in the case of imbalanced datasets.\n",
    "- **Model Generalization**: By averaging predictions across all folds, I learned that combining predictions from multiple models helps in improving the robustness of the final prediction, reducing overfitting to a specific fold.\n",
    "- **AUC-ROC**: The use of AUC-ROC as the evaluation metric gave me a comprehensive view of the model’s ability to distinguish between classes, which is especially important when the classes are imbalanced.\n",
    "\n",
    "### **Conclusion**:\n",
    "- This approach helped me get more confidence in the model's performance by using **Stratified K-Fold** cross-validation and **AUC-ROC** as the evaluation metric.\n",
    "- I also learned that **OOF predictions** are crucial for obtaining more reliable model performance estimates, and they provide insight into how the model might generalize to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c01d6a52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c01d6a52",
    "outputId": "c27515aa-0046-4ece-c67a-42e87d586afb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "\n",
      "Fold 2\n",
      "\n",
      "Fold 3\n",
      "\n",
      "Fold 4\n",
      "\n",
      "Fold 5\n",
      "\n",
      "AUC-ROC: 0.9515332509742837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Initialize Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Arrays to collect OOF and test predictions\n",
    "oof_preds = np.zeros(X_processed.shape[0])\n",
    "test_preds = np.zeros(test_processed.shape[0])\n",
    "\n",
    "# Perform training and prediction\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_processed, y)):\n",
    "    print(f\"\\nFold {fold+1}\")\n",
    "    X_train_fold, X_val_fold = X_processed.iloc[train_idx], X_processed.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # Train the model\n",
    "    best_model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], verbose=False)\n",
    "\n",
    "    # Predict OOF and test set\n",
    "    oof_preds[val_idx] = best_model.predict_proba(X_val_fold)[:, 1]\n",
    "    test_preds += best_model.predict_proba(test_processed)[:, 1] / skf.n_splits\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nAUC-ROC:\", roc_auc_score(y, oof_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f40ec44",
   "metadata": {
    "id": "8f40ec44"
   },
   "source": [
    "## Model Evaluation\n",
    "\n",
    "The model's performance is evaluated using AUC-ROC. A higher AUC indicates better performance in distinguishing between stressed and non-stressed customers. In this case, the AUC-ROC score of 0.9515 indicates that the model performs exceptionally well, with its ability to correctly classify both the positive (stressed) and negative (non-stressed) classes.\n",
    "\n",
    "AUC-ROC score provides a comprehensive evaluation of the model's performance across all classification thresholds, making it especially useful in situations with imbalanced classes, as it measures both the true positive rate and false positive rate.\n",
    "\n",
    "The high AUC-ROC score of 0.9515 shows that the model can distinguish between stressed and non-stressed customers with high accuracy, making it an effective tool for identifying at-risk clients in the financial setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e056ed",
   "metadata": {
    "id": "30e056ed"
   },
   "source": [
    "## Final Submission\n",
    "\n",
    "We will prepare the final submission file containing the test IDs and predicted probabilities of the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6336beaa",
   "metadata": {
    "id": "6336beaa"
   },
   "outputs": [],
   "source": [
    "# Predict the probabilities\n",
    "test_preds_probs = best_model.predict_proba(test_processed)[:, 1]\n",
    "\n",
    "# Uncomment ONE of the following lines based on what you want to save:\n",
    "\n",
    "# Option 1: Save probabilities\n",
    "test_preds_final = test_preds_probs\n",
    "\n",
    "# Option 2: Save binary labels\n",
    "#test_preds_final = (test_preds_probs > 0.5).astype(int)\n",
    "\n",
    "# Prepare the final submission\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'target': test_preds_final\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv('submission_binary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b458a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
