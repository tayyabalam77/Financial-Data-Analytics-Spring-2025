# -*- coding: utf-8 -*-
"""Mohammad Tayyab Alam 24157 Assignment 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WTQ1SmL-xNVMjxqjoA_lARZ1a8WVqeS6

# Assignment 02 - Logistic Regression Model for Fraudulent Insurance Claims
# Prepared by: Mohammad Tayyab Alam
- ERP 24157

# **Acknowledgements**
- Generative AI models such as DeepSeek , OpenAI's ChatGPT 4o and Claude were used to improve the model.
- Assistance from generative AI was taken to fix python code errors.
- Assistance from generative AI was taken to write complex python codes.
- Ideas for feature engineering were discussed with AI.
- All analysis in the markdown format is my own original work and analysis of the code and my understanding of the model and the work that I have done.
- Attempts to improve the model and troubleshooting were a mixture of research on github , stackoverflow and kaggle alongside AI help.



---

- The Class collab file called 08/03_Nibaf_Ecommerce_fraud.ipynb was referred to create the undersampling code.
- Ideas for the layout of the notebook were taken as an inspiration from it
- The plotting code was referrenced but then AI was used to create a better loop that is dynamic to plot all confusion matrixes and box plots

#Introduction
This notebook involves the following (analysis are provided later in depth):
- **Data loading**  
  - Importing libraries
  - Importing data
- **Data Preprocessing and Exploratory Data Analysis (EDA)**
  - Printing information about the raw data set via df.info()
  - Checking for missing values
  - Summarizing the key statistics
  - Checking the target variable (Fraud) distribution
  - Plotting a box plot for all the numerical columns to identify outliers and data patterns.
  - A correlation heatmap was plotted to check for correlation between the numerical variables and the target variable (Fraud).
  - Identifying anomalies in data such as repeated vehicle number plate for different vehicle type. This is used to create a feature later.
  - Duplicate checking
  - Checking Unique Values
  - Dropping identifier columns
  - One Hot Encoding
- Model Training and Analysis before Feature Engineering
  - Stratified Test Train Split (70:30)
  - Conducting Raw Logistic Regression without Class Imbalance (Control)
  - SMOTE Imbalance Dealing Model Training
  - Automatic Undersampling (python built in code - suggested by OpenAI's Chat GPT 4o)
  - Cost Sensitive Model Training
  - Class Weight Model Training
  - Undersampling as per the method in 08/03_Nibaf_Ecommerce_fraud class lecture
- Feature Engineering
- Model Training and Analysis after Feature Engineering
  - Stratified Test Train Split (70:30)
  - Conducting Raw Logistic Regression without Class Imbalance with (Control)
  - SMOTE Imbalance Dealing Model Training
  - Automatic Undersampling (python built in code - suggested by OpenAI's Chat GPT 4o)
  - Cost Sensitive Model Training
  - Class Weight Model Training
  - Undersampling as per the method in 08/03_Nibaf_Ecommerce_fraud class lecture
  - Running the undersampled class model on full dataset
  - Creating a table of results
  - Printing the best model
  - Checking for Potential Data Leakage via Correlation
  - Regularization
- Attempts to Improve Model
  - Identifying the best possible thresholds based on goals
  - Adjusting the weight for minority class

# Data Loading

## Importing Libraries
- All the libraries that are necessary for this logistic regression model to run are imported
- The insurance claims data set is loaded
"""

# Importing all the libraries used in the entire notebook
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix
)

from sklearn.utils import resample
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.metrics import classification_report
from collections import Counter
from sklearn.metrics import roc_curve

# Loading the file from google collab upload option
file_path = "insurance_claims.csv"
df = pd.read_csv(file_path)

"""# Data Pre Processing and Exploratory Data Analysis (01 Mark)
- Data set info
  - We can see that the raw data set contains 26 columns out of which 25 are our potential predictors.
  - When we look closely we can immediately rule out PolicyholderNumber and ReferenceId column and drop it as they are irrelevant for predicting
  - Total potential predictors are now 23
  - There are 12 object data types. This means that they are strings / not in number format and we need to convert them via dummy variable method to make them compatible with logistic regression.
  - Float and integer columns should be checked for outliers.
- Missing Values
  - We have no missing values in any column which saves us time.
"""

# Basic Information & Missing Values
print("Dataset Info:")
df.info()
print("\nMissing Values:")
print(df.isnull().sum())

"""## Summary Statistics
## PolicyholderPostcode
  - This is a location based column which has post codes in number format. The summary statistic of this column is not relevant for analysis but has been included here for completeness.

## PolicyWasSubscribedOnInternet

  - The mean value shows that 29% of the policies were subscribed via internet and the remainder offline. This variable is worth keeping for fraud analysis as it can show trends in offline vs online fraud tendency.
  - It is a binary variable so min is 0 and max is 1

## NumberOfPoliciesOfPolicyholder

  - Most policyholders have 1 to 2 policies indicated by mean with the highest at 4 -> could be an outlier

## FpVehicleAgeMonths

  - Vehicles are on average 10 years old.
  - The range is from 1 month to nearly 20 years.

## PolicyHolderAge

- Policyholders are mostly middle-aged (33-64 years), with a minimum of 18 years.

## FirstPartyLiability

- Liability varies widely (range: 0.1 to 0.9)
- Average is almost 50% suggesting that 50% of liability falls on the client in most claims

## DamageImportance

- Average value is 5.

## EasinessToStage

- Shows how difficult it is to replicate a claim.

## ClaimWithoutIdentifiedThirdParty

- The summary shows that only very few claims were without an identified third party (self claims).

## ClaimAmount

- Average claim amount is $25208. Useful for setting a line between too much and too less.

## LossHour

- On average claims occur at 11pm

## NumberOfBodilyInjuries

- Most claims dont have bodily injuries shown by median. Some do which need to be looked at for exaggeration.

## Fraud

- Our target variable
"""

# Generating summary stats
print("\nSummary Statistics:")
print(df.describe())

"""## Class Distribution
- We can see that there is severe imbalance in the data with only 1% of the data being our target that we are trying to predict.
- There is very little data for the model to learn from regarding frauds.
- Undersampling or adjustments are important.
"""

# calculating class distribution
class_distribution = df['Fraud'].value_counts(normalize=True) * 100
print("\nClass Distribution:")
print(class_distribution)

"""## Plotting class distribution in bar chart"""

# Setting style for visualization for consistency
sns.set(style="whitegrid")

# graphinh class distribution
plt.figure(figsize=(6, 4))
sns.barplot(x=class_distribution.index, y=class_distribution.values, palette="coolwarm")
plt.xlabel("Fraud Classification")
plt.ylabel("Percentage")
plt.title("Class Distribution of Fraudulent vs. Non-Fraudulent Claims")
plt.xticks([0, 1], ["Non-Fraudulent", "Fraudulent"])
plt.show()

"""## Box Plots
- Box Plots help identify outliers such as those in number of bodily injuries

"""

# Creating a box plot for numerical columns in the data
numerical_columns = ['PolicyHolderAge', 'FpVehicleAgeMonths', 'ClaimAmount', 'LossHour', 'NumberOfBodilyInjuries']
plt.figure(figsize=(10, 6))
for i, column in enumerate(numerical_columns, 1):
    plt.subplot(2, 3, i)
    sns.boxplot(df[column])
    plt.title(f"Box Plot of {column}")
    plt.tight_layout()

plt.show()

"""## Correlation Heatmap
- We can see that there are no strong predictors in the data set.
- Feature engineering can adress this.
"""

# Creating a correl heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(numeric_only=True), cmap="vlag", annot=True, fmt=".2f", linewidths=0.5, center=0)
plt.title("Feature Correlation Heatmap")
plt.show()

"""## Identifying an anomaly
- Checking if the same number plate is being used for different vehicle type?
- if yes identify this as a descrepency to avoid problems in future feature engineering
"""

# Investigating inconsistencies in First Party Vehicle Type
first_party_inconsistencies = (
    df.groupby('FirstPartyVehicleNumber')['FirstPartyVehicleType']
    .nunique()
    .reset_index()
)

# renaming the columns
first_party_inconsistencies.columns = ['FirstPartyVehicleNumber', 'UniqueVehicleTypes']

# filtering for vehciles sharing the same number plates
inconsistent_vehicles = first_party_inconsistencies[first_party_inconsistencies['UniqueVehicleTypes'] > 1]

# output
if not inconsistent_vehicles.empty:
    print("The following FirstPartyVehicleNumbers are associated with multiple vehicle types:")
    print(inconsistent_vehicles)
    print(f"\nTotal count of inconsistent FirstPartyVehicleNumbers: {len(inconsistent_vehicles)}")
else:
    print("No FirstPartyVehicleNumbers are associated with multiple vehicle types.")

"""## Checking duplicates"""

# Checking duplicates
duplicate_summary = df.apply(lambda x: x.duplicated().sum())
print("Duplicate counts per column:\n", duplicate_summary)

"""## Checking Unique Values"""

# Checking the cardinality of categorical variables
categorical_columns = df.select_dtypes(include=['object']).columns

# checkign the unique values
cardinality = df[categorical_columns].nunique()
print(cardinality)

"""- We can't one hot encode highly cardinal columns so we will need to drop them.
- Threshold set at 10 unique items
"""

# Identifying variables with high cardinality (I have defined it as being more than 10)
high_cardinality_columns = cardinality[cardinality > 10]

high_cardinality_columns

"""## Dropping the high cardinality columns"""

# Dropping high cardinal columns
data=df.copy()
drop_columns = ['PolicyholderNumber', 'FirstPartyVehicleNumber', 'ReferenceId', 'ThirdPartyVehicleNumber','LossDate']
data_cleaned = data.drop(columns=drop_columns)

"""## Selecting variables to encode
- Logistic regression requires numerical data so catagorical are converted to dummy
"""

#anything below or equal to 10 is low cardinality
low_cardinality_columns = cardinality[cardinality <=10 ]
print(low_cardinality_columns)

"""## Connection Between Parties is Yes or No so converting it to 0 or 1"""

# Converting the 'ConnectionBetweenParties' column to binary (1 for 'Yes', 0 for 'No')
data_cleaned['ConnectionBetweenParties'] = data['ConnectionBetweenParties'].apply(lambda x: 1 if x == 'Yes' else 0)

print(data_cleaned[['ConnectionBetweenParties']].head())

# One-hot encoding
variables_to_encode = ['PolicyholderOccupation', 'ClaimInvolvedCovers', 'FirstPartyVehicleType', 'InsurerNotes', 'ClaimCause']
data_encoded = pd.get_dummies(data_cleaned, columns=variables_to_encode, drop_first=True, dtype=int)

data_encoded

"""# Handling Class Imbalance (02 Marks) + Logistic Regression Models (02 Marks) [4 Marks Total]

## Model Training and Analysis before Feature Engineering

### Defining X and Y and dropping irrelevant variables and the target from X
- Using stratified test train split at 70:30
- Stratified used to address class imbalance
- Printing the test and train sizes
"""

X = data_encoded.drop(columns=['Fraud', 'FirstPolicySubscriptionDate'])
y = data_encoded['Fraud']

# Train-test split with stratification
X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Print the size of each split
print(f"Size of X_train: {X_train_raw.shape}")
print(f"Size of X_test: {X_test_raw.shape}")
print(f"Size of y_train: {y_train_raw.shape}")
print(f"Size of y_test: {y_test_raw.shape}")

"""### Training the logistic regression model without class imbalance and feature engineering to act as control for testing the impact of class imbalance and feature engineering"""

# Control Model - Log reg without imbalance or features
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_raw, y_train_raw)
y_pred_raw_noimbalance = log_reg.predict(X_test_raw)

"""### Defining a function to calculate the key metrics to evaluate every model based on accuracy , precission , recall (our top priority) , f1 score, auc and roc and plotting a confusion matrix"""

# function for metrics
def evaluate_model(y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    auc_roc = roc_auc_score(y_true, y_pred)
    cm = confusion_matrix(y_true, y_pred)
    return accuracy, precision, recall, f1, auc_roc, cm

"""### Evaluating the logistic model and storing results in a list which will be converted to dataframe later for easier comparission!
- We can see that the logistic regression model without any imbalance handling and feature engineering is unable to predict any fraud cases.
- Even thought the accuracy is 99% it is irrelevant to us as we would loose money due to fraud which would go unnoticed
"""

# evaluating the model
acc, prec, rec, f1, auc, cm = evaluate_model(y_test_raw, y_pred_raw_noimbalance)
results = []
# Print each evaluation metric
print(f"Accuracy: {acc}")
print(f"Precision: {prec}")
print(f"Recall: {rec}")
print(f"F1-Score: {f1}")
print(f"AUC-ROC: {auc}")

# results being stores in list to be used later
results_default = {
    'Method': 'No Class Imbalance (No Feature Engineering)',
    'Accuracy': acc,
    'Precision': prec,
    'Recall': rec,
    'F1-Score': f1,
    'AUC-ROC': auc
}
results.append(results_default)

"""### Logistic Regression Confusion Matrix
- We can see that the model 100% predicted all true negatives and also had false negatives on all the fraud cases.
- The model was unable to predict any fraud
- This is a very poor model and is our control
"""

# confusion matrix
cm = confusion_matrix(y_test_raw, y_pred_raw_noimbalance)

# classification report
print("Classification Report:")
print(classification_report(y_test_raw, y_pred_raw_noimbalance))

# Plotting the matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraudulent', 'Fraudulent'], yticklabels=['Non-Fraudulent', 'Fraudulent'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""### Synthetic Minority Oversampling Technique
- Now we will use SMOTE to see if imbalance handling by synthetic data oversampling is going to help address this class imbalance
- It generates synthetic data and the 1 means that 100% of 0's will be 1.
- For example out of 100 0's 100 will be 1 so total size will be 200 rows
"""

# creating smote model
smote = SMOTE(random_state=42, sampling_strategy=1)
X_train_smote, y_train_smote = smote.fit_resample(X_train_raw, y_train_raw)

# training
log_reg.fit(X_train_smote, y_train_smote)
y_pred_smote = log_reg.predict(X_test_raw)

# checking the new synthetic data
class_counts = Counter(y_train_smote)
print(f"Class distribution after undersampling: {class_counts}")

"""### Evaluating the model
- We can see that SMOTE model is a little better.
- This shows class imbalance handling is an important value addition.

"""

# evaluation
acc_smote, prec_smote, rec_smote, f1_smote, auc_smote, cm_smote = evaluate_model(y_test_raw, y_pred_smote)

# append to list
results_smote = {
    'Method': 'SMOTE (No Feature Engineering)',
    'Accuracy': acc_smote,
    'Precision': prec_smote,
    'Recall': rec_smote,
    'F1-Score': f1_smote,
    'AUC-ROC': auc_smote
}
results.append(results_smote)
# metrics display
print(f"Accuracy: {acc_smote}")
print(f"Precision: {prec_smote}")
print(f"Recall: {rec_smote}")
print(f"F1-Score: {f1_smote}")
print(f"AUC-ROC: {auc_smote}")

"""### Synthetic Minority Oversampling Technique Confusion Matrix
- We can see that the tuples that the model predicts as fraud 7 were infact true positives while 377 were false postiives.
"""

cm = confusion_matrix(y_test_raw, y_pred_smote)

#classification report
print("Classification Report:")
print(classification_report(y_test_raw, y_pred_smote))

# create confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraudulent', 'Fraudulent'], yticklabels=['Non-Fraudulent', 'Fraudulent'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""### Training undersampling model (automatic undersampling)
- Automatically reduces the major class in a 1 to 1 ratio with the minor class (1) in our case.
- 70:30 split resulted in 70 (0) rows and 70 (1) rows with a train set of 140 rows and 30 rows of test
- No synthetic data is generated here
"""

# automatic undersample with 1:1 ( 1 fraud for 1 non fraud)
undersample = RandomUnderSampler(sampling_strategy=1, random_state=42)
X_train_undersample, y_train_undersample = undersample.fit_resample(X_train_raw, y_train_raw)

# checking the new data
class_counts = Counter(y_train_undersample)
print(f"Class distribution after undersampling: {class_counts}")

"""### Predicting and viewing the probabilities of classes
- We can adjust the prediction threshold here if needed
"""

# train model
log_reg.fit(X_train_undersample, y_train_undersample)

# prediction probabilities (Class 1)
y_pred_probabilities = log_reg.predict_proba(X_test_raw)[:, 1]

# Set the threshold (freedom to explore but I did not change this for like to like)
threshold = 0.5

# predictions on adjustment threshold
y_pred_undersample = (y_pred_probabilities >= threshold).astype(int)

# results
print("Prediction Probabilities (Class 1):")
print(y_pred_probabilities)

print("\nAdjusted Predictions (Threshold = {:.2f}):".format(threshold))
print(y_pred_undersample)

"""### Model Evaluation - Undersampling
- It has higher recall but lower f1 than SMOTE
"""

# undersample model evaluation
acc_undersample, prec_undersample, rec_undersample, f1_undersample, auc_undersample, cm_undersample = evaluate_model(y_test_raw, y_pred_undersample)

# append in list and calculate
results_undersample = {
    'Method': 'Automatic Undersampling (No Feature Engineering)',
    'Accuracy': acc_undersample,
    'Precision': prec_undersample,
    'Recall': rec_undersample,
    'F1-Score': f1_undersample,
    'AUC-ROC': auc_undersample
}
results.append(results_undersample)

# results
print(f"\nAutomatic Undersampling Results (No Feature Engineering):")
print(f"Accuracy: {acc_undersample:.4f}")
print(f"Precision: {prec_undersample:.4f}")
print(f"Recall: {rec_undersample:.4f}")
print(f"F1-Score: {f1_undersample:.4f}")
print(f"AUC-ROC: {auc_undersample:.4f}")
print(f"Confusion Matrix: \n{cm_undersample}\n")

"""### Undersampling - Automatic - Confusion Matrix
- We can see that the model has predicted 5 true positives with 1231 false positives (an operational cost in this case) while it has predicted 25 false negatives (it does not predict these as frauds)

"""

# matrix
cm = confusion_matrix(y_test_raw, y_pred_undersample)

# report
print("Classification Report:")
print(classification_report(y_test_raw, y_pred_undersample))

# output
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraudulent', 'Fraudulent'], yticklabels=['Non-Fraudulent', 'Fraudulent'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""### Training cost sensitive model
- This model applies a penalty for misclassifying the target class of 1.
- Makes it so that the model becomes cautious of causing errors in the high penalty weight class.
- 100 penalty for wrong in 1 , 1 for 0
"""

# training Manual Cost-Sensitive Learning (Class-Weight Adjustments) on raw data
log_reg_raw = LogisticRegression(class_weight={0: 1, 1: 100}, max_iter=1000)

# fit model
log_reg_raw.fit(X_train_raw, y_train_raw)

# predicting

y_pred_final = log_reg_raw.predict(X_test_raw)

"""### Cost Sensitive Model Evaluation"""

# Evaluate  Cost-Sensitive Learning (Raw Data)
acc_final, prec_final, rec_final, f1_final, auc_final, cm_final = evaluate_model(y_test_raw, y_pred_final)

# collecting results and appending
results_cost_sensitive = {
    'Method': 'MANUAL Cost-Sensitive Learning (Raw Data)',
    'Accuracy': acc_final,
    'Precision': prec_final,
    'Recall': rec_final,
    'F1-Score': f1_final,
    'AUC-ROC': auc_final
}


results.append(results_cost_sensitive)

#output
print(f"\nCost-Sensitive Learning Results (Raw Data):")
print(f"Accuracy: {acc_final:.4f}")
print(f"Precision: {prec_final:.4f}")
print(f"Recall: {rec_final:.4f}")
print(f"F1-Score: {f1_final:.4f}")
print(f"AUC-ROC: {auc_final:.4f}")
print(f"Confusion Matrix: \n{cm_final}\n")

"""### Confusion Matrix - Cost Sensitive Model"""

#matrix
cm = confusion_matrix(y_test_raw, y_pred_final)
#report
print("Classification Report:")
print(classification_report(y_test_raw, y_pred_final))

#output
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraudulent', 'Fraudulent'], yticklabels=['Non-Fraudulent', 'Fraudulent'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""###  Manual Undersampling as per class method
- In our 4th code discussed on 8 March 2025 the undersampling was done by the method below
- We first separate the fraud and non fraud and then merge them as per the length of the fraud (100 in this case) and create a balanced data set
- Then split into train and test (70:30)
- Create a model
- The model is trained on the balanced data set but predicts on the FULL RAW DATA SET for comparability
"""

# separating fraud and non fraud
df_fraud = data_encoded[data_encoded['Fraud'] == 1]
df_non_fraud = data_encoded[data_encoded['Fraud'] == 0]

# Undersample the non-fraud class to match the number of fraud cases
df_non_fraud_sample = df_non_fraud.sample(n=len(df_fraud), random_state=42)

# Combine the fraud and undersampled non-fraud cases
df_balanced = pd.concat([df_fraud, df_non_fraud_sample])

X_balanced = df_balanced.drop(columns=['Fraud', 'FirstPolicySubscriptionDate'])
y_balanced = df_balanced['Fraud']
# definining variables for x and y

# Split data in train and test
X_train_balanced, X_val_balanced, y_train_balanced, y_val_balanced = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)

"""#### Checking size of the data set"""

# checking data
print(f"Number of rows and columns in X_train_balanced: {X_train_balanced.shape}")
print(f"Number of rows and columns in Y_train_balanced: {y_train_balanced.shape}")

# Train
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_balanced, y_train_balanced)

# predict on full raw set
y_pred_balanced = log_reg.predict(X_train_raw)

"""###  Class Undersample Model Evaluation
This model has good recall but at the expense of false positives,
"""

# model evaluation
acc_balanced, prec_balanced, rec_balanced, f1_balanced, auc_balanced, cm_balanced = evaluate_model(y_train_raw, y_pred_balanced)

# matrix
print(f"Confusion Matrix:\n{cm_balanced}\n")

# report
print("Classification Report:")
print(classification_report(y_train_raw, y_pred_balanced))

# collecting result and appending
results_balanced = {
    'Method': 'Undersampling - Class Method FULL DATA',
    'Accuracy': acc_balanced,
    'Precision': prec_balanced,
    'Recall': rec_balanced,
    'F1-Score': f1_balanced,
    'AUC-ROC': auc_balanced
}

results.append(results_balanced)

"""### Class Undersample Model Confusion Matrix"""

cm = cm_balanced
print("Classification Report:")
print(classification_report(y_train_raw, y_pred_balanced))

# matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraudulent', 'Fraudulent'], yticklabels=['Non-Fraudulent', 'Fraudulent'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""## Feature Engineering (AI Assisted Content)

- Data was reloaded to ensure no corruption

| **Feature Name**                     | **Description** |
|--------------------------------------|----------------|
| **ClaimAgeRatio** | Compares the policyholder's age to the vehicle's age. |
| **ClaimAmountPerPolicy** | Shows how much claim amount is being made per policy owned by the policyholder. |
| **DamageImportanceToClaimAmountRatio** | Measures the reported damage importance relative to the claim amount. Higher values may suggest exaggerated claims. |
| **PolicySubscriptionDuration** | Calculates how long the policy has been active in days. Older policies may have different fraud patterns. |
| **TimeBetweenClaims** | Measures the gap (in days) between successive claims on the same vehicle. Frequent claims might indicate fraud. |
| **ClaimFrequency** | Compares the number of policies to the claim amount. Higher values might indicate suspiciously frequent claims. |
| **ClaimWithoutThirdPartyRatio** | Measures the proportion of claims that were reported without an identified third party, which could indicate staged accidents. |
| **PolicyholderRisk** | A rough risk score based on the policyholder’s age and the number of policies they hold. |
| **ClaimAmountPerBodilyInjury** | Shows how much claim amount is being made per reported bodily injury. Unusually high values could indicate exaggerated injuries. |
| **ClaimHourBinned** | Groups the time of loss into four categories (Night, Morning, Afternoon, Evening) to analyze fraud patterns at different times of the day. |
| **ClaimAmountCategorical** | Divides claim amounts into Low, Medium, High, and Very High categories for better pattern analysis. |
| **OldVehicleFlag** | Flags whether the vehicle is older than 10 years. Older vehicles might be linked to more fraudulent claims. |
| **HighClaimAmountFlag** | Identifies the top 25% of claims based on amount. Higher-value claims are often more scrutinized for fraud. |
| **WeekendOrHolidayLoss** | Marks whether the loss happened on a weekend or public holiday. Fraud might be more likely when fewer witnesses are around. |
| **LossAtOddHours** | Flags claims that happened between midnight and 4 AM, which could indicate staged accidents. |
| **PolicyTenureCategory** | Categorizes how long the policy has been active: New, 1-3 Years, or 3+ Years. Newer policies might have different fraud risks. |
| **PolicyAgeGroup** | Categorizes policyholders as Young, Middle-aged, or Senior to study fraud patterns across different age groups. |

"""

# loading again to avoid corruption
file_path = "insurance_claims.csv"
df = pd.read_csv(file_path)

# Convert date columns to datetime
df['FirstPolicySubscriptionDate'] = pd.to_datetime(df['FirstPolicySubscriptionDate'])
df['LossDate'] = pd.to_datetime(df['LossDate'])

# Feature 1: ClaimAgeRatio - Policyholder age relative to vehicle age
df['ClaimAgeRatio'] = df['PolicyHolderAge'] / (df['FpVehicleAgeMonths'] + 1)

# Feature 2: ClaimAmountPerPolicy - Claim amount divided by number of policies
df['ClaimAmountPerPolicy'] = df['ClaimAmount'] / (df['NumberOfPoliciesOfPolicyholder'] + 1)

# Feature 3: DamageImportanceToClaimAmountRatio
df['DamageImportanceToClaimAmountRatio'] = df['DamageImportance'] / (df['ClaimAmount'] + 1)

# Feature 4: PolicySubscriptionDuration - Number of days since policy subscription
df['PolicySubscriptionDuration'] = (pd.to_datetime('today') - df['FirstPolicySubscriptionDate']).dt.days

# Feature 5: TimeBetweenClaims - Days between claims for the same vehicle
df['TimeBetweenClaims'] = df.groupby(['FirstPartyVehicleNumber', 'FirstPartyVehicleType'])['LossDate'].diff().dt.days.abs()
df['TimeBetweenClaims'] = df['TimeBetweenClaims'].fillna(0)

# Feature 6: ClaimFrequency - Number of policies per claim
df['ClaimFrequency'] = df['NumberOfPoliciesOfPolicyholder'] / (df['ClaimAmount'] + 1)

# Feature 7: ClaimWithoutThirdPartyRatio - Ratio of self-reported claims
df['ClaimWithoutThirdPartyRatio'] = df['ClaimWithoutIdentifiedThirdParty'] / (df['ClaimAmount'] + 1)

# Feature 8: PolicyholderRisk - Combined risk metric
df['PolicyholderRisk'] = df['PolicyHolderAge'] * (df['NumberOfPoliciesOfPolicyholder'] + 1)

# Feature 9: ClaimAmountPerBodilyInjury - Claim amount per injury
df['ClaimAmountPerBodilyInjury'] = df['ClaimAmount'] / (df['NumberOfBodilyInjuries'] + 1)

# Feature 10: ClaimHourBinned - Categorize loss hour
bins = [0, 6, 12, 18, 24]
labels = ['Night', 'Morning', 'Afternoon', 'Evening']
df['ClaimHourBinned'] = pd.cut(df['LossHour'], bins=bins, labels=labels, right=False)

# Feature 11: ClaimAmountCategorical - Categorize claim amounts
bins = [0, 5000, 20000, 50000, np.inf]
labels = ['Low', 'Medium', 'High', 'Very High']
df['ClaimAmountCategorical'] = pd.cut(df['ClaimAmount'], bins=bins, labels=labels)

# Feature 12: OldVehicleFlag - If vehicle age >10 years
df['OldVehicleFlag'] = df['FpVehicleAgeMonths'] / 12 > 10

# Feature 13: HighClaimAmountFlag - Top 25% of claims
df['HighClaimAmountFlag'] = df['ClaimAmount'] > df['ClaimAmount'].quantile(0.75)

# Feature 14: WeekendOrHolidayLoss - If claim occurred on weekend/public holiday
df['WeekendOrHolidayLoss'] = df['LossDate'].dt.dayofweek >= 5

# Feature 15: LossAtOddHours - If loss happened between 12 AM - 4 AM
df['LossAtOddHours'] = df['LossHour'].between(0, 4)

# Feature 16: PolicyTenureCategory - Categorizing policy tenure
df['PolicyTenureCategory'] = pd.cut(df['PolicySubscriptionDuration'],
                                    bins=[-1, 365, 1095, float('inf')],
                                    labels=['New', '1-3 Years', '3+ Years'])

# Feature 17: PolicyAgeGroup - Categorizing policyholder age
df['PolicyAgeGroup'] = pd.cut(df['PolicyHolderAge'], bins=[-1, 30, 50, float('inf')],
                              labels=['Young', 'Middle-aged', 'Senior'])

# Drop redundant columns
columns_to_drop = [
    'EasinessToStage', 'NumberOfBodilyInjuries', 'ClaimAmount', 'LossHour',
    'FpVehicleAgeMonths', 'NumberOfPoliciesOfPolicyholder', 'PolicyHolderAge',
    'FirstPolicySubscriptionDate', 'LossDate'
]
df_cleaned = df.drop(columns=columns_to_drop)

# Show dataset info after feature engineering
df_cleaned.info()

"""### Checking missing values in these new fetures"""

# Check for missing values
missing_values = df.isnull().sum()

# Print the result
print("Missing Values in Each Column:\n", missing_values)

"""### Checking cardinality to decide which columns to drop and encode"""

# checking cardinality
categorical_columns = df_cleaned.select_dtypes(include=['object']).columns

# catagorical cols
cardinality = df_cleaned[categorical_columns].nunique()
print(cardinality)

# bring clean back to df
df = df_cleaned

# drop irrelevant columns which are highly unique values
drop_columns = [
                'FirstPartyVehicleNumber', 'ThirdPartyVehicleNumber','PolicyholderNumber','ReferenceId']
df = df.drop(columns=drop_columns)

# Convert 'ConnectionBetweenParties' column yes no to 1 0
df['ConnectionBetweenParties'] = df['ConnectionBetweenParties'].apply(lambda x: 1 if x == 'Yes' else 0)

# One-hot encode
variables_to_encode = [
    'PolicyholderOccupation', 'ClaimInvolvedCovers', 'FirstPartyVehicleType',
    'InsurerNotes', 'ClaimCause','PolicyAgeGroup','PolicyTenureCategory','ClaimAmountCategorical','ClaimHourBinned'
]

df_encoded = pd.get_dummies(df, columns=variables_to_encode, drop_first=True, dtype=int)

"""###Checking if anything else needs to be converted after encoding"""

print(df_encoded.dtypes)  # checking data types

for col in df_encoded.select_dtypes(include=['object']).columns:
    print(f"Unique values in {col}:\n", df_encoded[col].unique(), "\n")

df_encoded

"""### Spliting the encoded data set into train and test using 70:30
- 70 30 used as imbalanced



"""

X_encoded = df_encoded.drop(columns=['Fraud'])  # Drop target column 'Fraud' so unseen
y_encoded = df_encoded['Fraud']  # Target column

# Stratified train-test split to maintain class balance in both sets
X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(
    X_encoded, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
)

"""### Printing the sizes
- We can see that the number of features (columns) has increased from 33 to 49 (33 was due to encoding)
"""

# Print the size of each split
print(f"Size of X_train: {X_train_final.shape}")
print(f"Size of X_test: {X_test_final.shape}")
print(f"Size of y_train: {y_train_final.shape}")
print(f"Size of y_test: {y_test_final.shape}")

"""### Repeating the class undersampling method"""

#  undersampling to balance the dataset
df_fraud = df_encoded[df_encoded['Fraud'] == 1]
df_non_fraud = df_encoded[df_encoded['Fraud'] == 0]
df_non_fraud_sample = df_non_fraud.sample(n=len(df_fraud), random_state=42)
df_balanced = pd.concat([df_fraud, df_non_fraud_sample])

# defining x and y and making sure model doesnt see fraud
X_balanced = df_balanced.drop(columns=['Fraud'])
y_balanced = df_balanced['Fraud']

# Train-test split (70-30)
X_train_bal_featured, X_test_bal_featured, y_train_bal_featured, y_test_bal_featured = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)
X_train_balanced_featured = X_train_bal_featured
X_test_balanced_featured = X_test_bal_featured

# Print the size of each split
print(f"Size of X_train: {X_train_bal_featured.shape}")
print(f"Size of X_test: {X_test_bal_featured.shape}")
print(f"Size of y_train: {y_train_bal_featured.shape}")
print(f"Size of y_test: {y_test_bal_featured.shape}")

"""###  Training  Class method Undersampling Model"""

# Train logistic regression model
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_balanced_featured, y_train_bal_featured)

"""###  Predictions
- We predict on both the full data and the undersampled data of 40 rows
"""

# Predictions
y_pred_bal_featured = log_reg.predict(X_test_bal_featured)
y_prob_bal_featured = log_reg.predict_proba(X_test_balanced_featured)[:, 1]
y_pred_bal_featured_fulldata = log_reg.predict(X_test_final)
y_prob_bal_featured_fulldata = log_reg.predict_proba(X_test_final)[:, 1]

# Initialize the model and list to store results
log_reg_final = LogisticRegression(max_iter=1000)
methods = [
    'No Class Imbalance but with Feature Engineering',
    'SMOTE (Feature Engineering)',
    'Automatic Undersampling (Feature Engineering)',
    'Undersampling - Class Method (Reference Nibaf Ecommerce Fraud - Feature Engineering)',
    'Class-Weight Adjustments (Feature Engineering)',
    'MANUAL Cost-Sensitive Learning (Feature Engineering)'
]

"""### Master Code for looping through all the models on the new feature engineered data
- Code calculates the metrics using the eval function defined earlier
- Code then merges all into results
- Then works on the manually trained model earlier in the previous cells and merges it

"""

# Dictionary to store confusion matrices & predictions
conf_matrices = {}
model_predictions = {}

# Loop through all methods
for best_method in methods:

    if best_method == 'SMOTE (Feature Engineering)':
        smote = SMOTE(random_state=42, sampling_strategy=0.55)
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_final, y_train_final)

    elif best_method == 'Automatic Undersampling (Feature Engineering)':
        undersample = RandomUnderSampler(random_state=42)
        X_train_resampled, y_train_resampled = undersample.fit_resample(X_train_final, y_train_final)

    elif best_method == 'MANUAL Cost-Sensitive Learning (Feature Engineering)':
        log_reg_final = LogisticRegression(class_weight={0: 1, 1: 100}, max_iter=1000)
        X_train_resampled, y_train_resampled = X_train_final, y_train_final  # No resampling

    elif best_method == 'No Class Imbalance but with Feature Engineering':
        X_train_resampled, y_train_resampled = X_train_final, y_train_final  # No resampling
        log_reg_final = LogisticRegression(max_iter=1000)  # Standard logistic regression

    else:
        continue  # we want to skip model training for 'Undersampling - Class Method' as we already trained it above

    # Train the model
    log_reg_final.fit(X_train_resampled, y_train_resampled)

    # Make predictions
    y_pred = log_reg_final.predict(X_test_final)

    # Store predictions and confusion matrix
    model_predictions[best_method] = y_pred
    conf_matrices[best_method] = confusion_matrix(y_test_final, y_pred)

    # Evaluate model
    acc, prec, rec, f1, auc, cm = evaluate_model(y_test_final, y_pred)

    # Store results for comparison
    results.append({
        'Method': best_method,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1-Score': f1,
        'AUC-ROC': auc
    })

# merging the already trained model
method_name = 'Undersampling - Class Method (Reference Nibaf Ecommerce Fraud - Feature Engineering)'

# Use stored predictions and test data
acc, prec, rec, f1, auc, cm = evaluate_model(y_test_bal_featured, y_pred_bal_featured)

# Store confusion matrix
conf_matrices[method_name] = cm

# Store results for comparison
results.append({
    'Method': method_name,
    'Accuracy': acc,
    'Precision': prec,
    'Recall': rec,
    'F1-Score': f1,
    'AUC-ROC': auc
})

"""### Merging using the metrics on full data of the undersample model"""

#adding the full data model (reused the code above)
method_name = 'Undersampling FULL DATA - Class Method (Reference Nibaf Ecommerce Fraud - Feature Engineering)'

# Use stored predictions and test data
acc, prec, rec, f1, auc, cm = evaluate_model(y_test_final, y_pred_bal_featured_fulldata)

# Store confusion matrix
conf_matrices[method_name] = cm

# Store results for comparison
results.append({
    'Method': method_name,
    'Accuracy': acc,
    'Precision': prec,
    'Recall': rec,
    'F1-Score': f1,
    'AUC-ROC': auc
})

"""# Model Evaluation (3 Marks)
- Confusion Matrix Analysis
- Model Performance and metrics Analysis (table below)

## Confusion Matrix Analysis

For the analysis below:
- FP means false positives which means that the model incorrectly predicts fraud when in reality it was not fraud .
- FN means false negatives which means that the model incorrectly predicts that there is no fraud when in reality it was fraud.
- Both FP and FN are a trade off.

## No Class Imbalance but with Feature Engineering

- FP: 0
- FN: 30
- Explanation: The model completely ignores fraudulent cases, classifying all as non-fraudulent. This means that the model is unable to predict any fraud cases

## SMOTE (Feature Engineering)

- FP: 92
- FN: 29
- Explanation: Improves recall slightly from without class imbalance model but it is still struggling to correctly classify fraudulent cases.

## Automatic Undersampling (Feature Engineering)

- FP: 1312
- FN: 18
- Explanation: This model trades off high false positives for better fraud detection. If the threshold is increased then there is a potential for improvement.

## Manual Cost-Sensitive Learning (Feature Engineering)

- FP: 1067
- FN: 18
- Explanation: This model is similar to the model above as it is having the same false negatives but lower false positives. Between the two this is the better model.

## Undersampling - Class Method (Reference Nibaf Ecommerce Fraud - Feature Engineering)

- FP: 15
- FN: 13
- Explanation: This is just to illustrate that the best model was trained on an undersampled data set and on that its performance was quite balanced.

## Undersampling FULL DATA - Class Method (Reference Nibaf Ecommerce Fraud - Feature Engineering)

- FP: 1533
- FN: 10
- Explanation: The best model predicts fraud good but it is too strict and produces a lot of false positives (misclassifies non fraud cases).

## Combined plots of Confusion Matrixes
"""

num_methods = len(conf_matrices)  # Total number of models
num_cols = 3  # Keep 3 columns for better visualization
num_rows = math.ceil(num_methods / num_cols)  # Dynamically adjust rows

fig, axes = plt.subplots(num_rows, num_cols, figsize=(18, 6 * num_rows))
axes = axes.flatten()  # Flatten for easier indexing

for i, (method, cm) in enumerate(conf_matrices.items()):
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraudulent', 'Fraudulent'],
                yticklabels=['Non-Fraudulent', 'Fraudulent'], ax=axes[i])
    axes[i].set_title(method)
    axes[i].set_xlabel('Predicted')
    axes[i].set_ylabel('Actual')

# Hide any unused subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""## Analysis of Table
### For the purpose of this analysis
- Precision means when fraud was predicted how accurate it was.
- Recall means how many of actual fraud cases where identified by the model.
- f1 is harmonic mean of precision and recall. Used as a decision criteria to select models.
- AUC-ROC is the model's ability to distinguish between classes at different thresholds. Higher the better.


---


- We can see from the table below that Class imbalance is extremely important to be able to predict fraud as Model 0 and Model 5 are unable to recall any fraud cases even with feature engineering. However, this model has the highest accuracy (99%) but it is irrelevant to us as we do not want to rely on this metric.
- SMOTE provides improvement in recall but at the expense of accuracy but it is not the best model. Moreover, surprisingly with feature engineering the model performance drops , this is attributable to the increase in features.
- The automatic undersampling sees a sharp decline in accuracy as the rows are sampled and not all rows are visible to the model (major class downsized) so the model looses accuracy. However, it has good increase in recall with feature engineering.
- Manual cost sensitive learning model has better accuracy than automatic and has similar recall with better f1 score
- However, the best model is undersampling which has the best recall 67% and the highest f1 score of 2.5%. It has quity low accuracy but as explained this is because it has not seen a lot of data and was  trained on a subset.

### Conclusive Summary
- Class Imbalance significantly improves recall (needed for predicting fraud in imbalanced data)
- Feature engineering improves recall and f1 score
- Feature engineering alone is not a substitute for class imbalance.
"""

# Convert results to a DataFrame for easy comparison
results_df = pd.DataFrame(results)
results_df

"""## Selecting the best model based on F1-Score"""

# Selecting the best method based on F1-Score
best_row = results_df.loc[results_df['F1-Score'].idxmax()]

# Extract values
best_method = best_row['Method']
best_accuracy = best_row['Accuracy']
best_precision = best_row['Precision']
best_recall = best_row['Recall']
best_f1 = best_row['F1-Score']
best_auc = best_row['AUC-ROC']

# Print the best method and its performance indicators
print(f"Best Method for Handling Class Imbalance: {best_method}")
print(f"Accuracy: {best_accuracy:.4f}")
print(f"Precision: {best_precision:.4f}")
print(f"Recall: {best_recall:.4f}")
print(f"F1-Score: {best_f1:.4f}")
print(f"AUC-ROC: {best_auc:.4f}")

"""## INVESTIGATING POTENTIAL DATA LEAKAGE
- There appears to be no variable with data leakage as all are low correlated with fraud.
- However we can use this matrix as a basis to opt for regularization
- We can see that there are very few correlated features with Fraud
- We will now conduct lasso l1 regularization below to remove these irrelevant features from our best model selected above
"""

# Combine Features and Target for correlation analysis
full_data = pd.concat([X_train_final, y_train_final], axis=1)

# Compute correlation matrix
corr_matrix = full_data.corr()

# Sort correlations with the target variable (Fraud)
target_corr = corr_matrix[y_train_final.name].drop(y_train_final.name).sort_values(ascending=False)

# Print top correlated features
print("Top features correlated with Fraud:\n", target_corr.head(10))

# Plot heatmap for correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', linewidths=0.5, center=0)
plt.title("Feature Correlation Heatmap")
plt.show()

"""## Regularization of L1 to remove irrelevant features as per above
- The model is trained below
- The confusion metric, roc and auc curves are plotted
"""

# Train logistic regression model with L1 regularization
log_reg = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)
log_reg.fit(X_train_balanced_featured, y_train_bal_featured)

# Make predictions on the test data
y_pred = log_reg.predict(X_test_final)

"""## Analysis of regularized model
- We can see that the model recall improves to 70% at the expense of false positives
- The model AUC is around 59% (close to 60%) which means that it is only ~ 10% better than a random guessing model at distinguishing between classes.
- This problem is happening due to the severe imbalance in data.
- However, lasso regularization does improve our best model's AUC
- False positives increase but False Negatives decrease (tradeoff) compared to non regularized model ( 1543 false positives by l1 vs 1533 by non l1 AND 9 false negatives by l1 vs 10 false negatives by non l1)
"""

# Generate the confusion matrix
cm = confusion_matrix(y_test_final, y_pred)

# Print the classification report
print("Classification Report:")
print(classification_report(y_test_final, y_pred))

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Non-Fraudulent', 'Fraudulent'],
            yticklabels=['Non-Fraudulent', 'Fraudulent'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Calculate the AUC-ROC score
y_pred_prob = log_reg.predict_proba(X_test_final)[:, 1]  # GETTING PROBABILITIES ONLY FOR THE POSITIVE CLASS HERE
auc = roc_auc_score(y_test_final, y_pred_prob)
print(f"AUC-ROC Score: {auc:.4f}")

# Plot the AUC-ROC curve
fpr, tpr, thresholds = roc_curve(y_test_final, y_pred_prob)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'AUC = {auc:.4f}')
plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Classifier')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')

"""# Insights and Recommendations (2 Marks)

## Summary of Model Performance and Strengths (Discussed earlier)
- The best model was undersampling by creating balanced data sets manually as taught in class.
- This model initially had an AUC of 57.5% so it was only 7.5% better than a random guesser at distinguishing between classes.
- This model had the best in class performance with respect to recall being 66.67% and f1 being 2.5%
- This best model was further improved by lasso l1 regularization as there were many irrelevant features.
- The best model now has a respectable 59.7% AUC and 0.03 f1 score with 70% recall.
- The model has the key strength of being able to identify fraud cases despite the highly imbalanced data set
- However , the model is limited by the data set as there is only 1% of fraud data available. More data will improve the model

## Model Improvements Suggestions
- In cases of high imbalance it is better to use models that boost variables such as xgboost and catboost models.
- Random ensemble trees can also identify trends
- Anomaly detection is good for identifying rare occurences and fraud appears to be a rare occurence in this data set.
- Adjusting threshold for this model can improve performance but there will be a trade off (this is demonstrated by the code below)
- Parameter tuning for SMOTE and under sampling can help ( by deciding the best sampling strategy)


---

##**The suggestion of threshold has been demonstrated below**

### Trying to improve the undersampled model on full data
- The chunk of codes below is basically an attempt to find the best threshold with different priorities
- In the first our priority is to maximize the F1 Score
- In the second our priority is to maximize Recall
- In the third our priority is to minimize false positives while also focussing on recall
- In the final our prioritiy is to minimize false positives without a concern for recall
"""

best_threshold = 0.5
best_f1 = 0

for threshold in np.arange(0.05, 0.95, 0.05):
    y_preds_adjusted = (y_prob_bal_featured_fulldata >= threshold).astype(int)
    f1 = f1_score(y_test_final, y_preds_adjusted)

    if f1 > best_f1:
        best_f1 = f1
        best_threshold = threshold

# Apply best threshold
y_preds_final = (y_prob_bal_featured_fulldata >= best_threshold).astype(int)
cm = confusion_matrix(y_test_final, y_preds_final)

# Print F1 and confusion matrix
print(f"Best F1-Score: {best_f1:.4f} at Threshold: {best_threshold:.2f}")
print("Confusion Matrix:\n", cm)

# Classification report
report = classification_report(y_test_final, y_preds_final)
print("\nClassification Report:\n", report)

best_threshold = 0.5
best_recall = 0
min_f1 = 0.5

for threshold in np.arange(0.05, 0.95, 0.05):
    y_preds_adjusted = (y_prob_bal_featured_fulldata >= threshold).astype(int)
    recall = recall_score(y_test_final, y_preds_adjusted)
    f1 = f1_score(y_test_final, y_preds_adjusted)

    if recall > best_recall and f1 >= min_f1:
        best_recall = recall
        best_threshold = threshold

# Apply best threshold
y_preds_final = (y_prob_bal_featured_fulldata >= best_threshold).astype(int)
cm = confusion_matrix(y_test_final, y_preds_final)

# Print Best Recall and Confusion Matrix
print(f"Best Recall: {best_recall:.4f} at Threshold: {best_threshold:.2f}")
print("Confusion Matrix:\n", cm)

# Classification report for detailed metrics
report = classification_report(y_test_final, y_preds_final)
print("\nClassification Report:\n", report)

best_threshold = 0.5
min_fp = float('inf')
min_recall = 0.5  # Ensure recall doesn't drop too low

for threshold in np.arange(0.05, 0.95, 0.05):
    y_preds_adjusted = (y_prob_bal_featured_fulldata >= threshold).astype(int)
    cm = confusion_matrix(y_test_final, y_preds_adjusted)
    fp = cm[0, 1]  # False Positives
    recall = recall_score(y_test_final, y_preds_adjusted)

    if fp < min_fp and recall >= min_recall:
        min_fp = fp
        best_threshold = threshold

# Apply the best threshold
y_preds_final = (y_prob_bal_featured_fulldata >= best_threshold).astype(int)
cm = confusion_matrix(y_test_final, y_preds_final)

# Print minimum false positives and confusion matrix
print(f"Minimum False Positives: {min_fp} at Threshold: {best_threshold:.2f}")
print("Confusion Matrix:\n", cm)

# Classification report for detailed metrics
report = classification_report(y_test_final, y_preds_final)
print("\nClassification Report:\n", report)

best_threshold = 0.5
min_fp = float('inf')

for threshold in np.arange(0.05, 0.95, 0.05):
    y_preds_adjusted = (y_prob_bal_featured_fulldata >= threshold).astype(int)
    cm = confusion_matrix(y_test_final, y_preds_adjusted)
    fp = cm[0, 1]  # False Positives

    if fp < min_fp:
        min_fp = fp
        best_threshold = threshold

# Apply the best threshold
y_preds_final = (y_prob_bal_featured_fulldata >= best_threshold).astype(int)
cm = confusion_matrix(y_test_final, y_preds_final)

# Print minimum false positives and confusion matrix
print(f"Minimum False Positives: {min_fp} at Threshold: {best_threshold:.2f}")
print("Confusion Matrix:\n", cm)

# Classification report for detailed metrics
report = classification_report(y_test_final, y_preds_final)
print("\nClassification Report:\n", report)

"""# References to Collab Files
https://colab.research.google.com/drive/1aq21zdPzg-bWOJwlawas8xo_N68LfyWw?usp=sharing#scrollTo=KpgHoBIkVzo7


https://colab.research.google.com/drive/1ChDhvV-q_5ao8TFdUTYUA3e5cr8FgrTy

https://colab.research.google.com/drive/1SSeQZYOzOQ1KjrK6PAIhuzKOcUIdgr_J

# Thank you for reading
## I acknowledge that I have taken assistance from generative AI models for solving complex coding problems and creating complex loops.
## To uphold academic integrity, I have attempted to understand the chunks of codes written and written analysis and interpretations myself.

- Mohammad Tayyab Alam
- ERP 24157
"""